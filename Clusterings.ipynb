{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from queue import PriorityQueue\n",
    "import math\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "stop_words =  set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(path):\n",
    "    with open(path) as file:\n",
    "        lines = file.read().replace('\\n', '')\n",
    "    \n",
    "    return nltk.sent_tokenize(lines)\n",
    "\n",
    "def filter_sentence(sentence):\n",
    "    tokens = gensim.utils.simple_preprocess(sentence)#nltk.tokenize.word_tokenize(sentence)\n",
    "    filtered = [w.lower() for w in tokens if not w in stop_words and w in word_model.vocab]\n",
    "    return filtered\n",
    "\n",
    "def get_sentence_embedding(sentence):\n",
    "    vectors = np.zeros((len(sentence),300))\n",
    "    for i, word in enumerate(sentence):\n",
    "        embedding = word_model[word]\n",
    "        assert np.all(np.isfinite(embedding))\n",
    "        vectors[i] = embedding\n",
    "    mean = np.mean(vectors, axis=0)\n",
    "    return mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_text(path):\n",
    "    sentences = get_sentences(path)\n",
    "    embeddings = []\n",
    "    for s in sentences:\n",
    "        filtered = filter_sentence(s)\n",
    "        if not len(filtered)==0:\n",
    "            embedded = get_sentence_embedding(filtered)\n",
    "            embeddings.append(embedded)\n",
    "    \n",
    "    embeddings_array = np.zeros((len(embeddings), 300))\n",
    "    for i,e in enumerate(embeddings):\n",
    "        embeddings_array[i] = e\n",
    "    return embeddings_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_embeddings(embeddings, components=150):\n",
    "    \n",
    "    covar =  PCA(n_components=components)\n",
    "    reduced = covar.fit_transform(embeddings)\n",
    "    return reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_vector(vectors):\n",
    "    #vectors are in a list\n",
    "    assert not len(vectors)==0\n",
    "    \n",
    "    first = np.zeros(vectors[0].shape)\n",
    "    for v in vectors:\n",
    "        first+=v\n",
    "    return first/(len(vectors))\n",
    "\n",
    "def mean_distances(matrix):\n",
    "    \n",
    "    total = 0\n",
    "    num = matrix.shape[0]*(matrix.shape[0] - 1) / 2 \n",
    "    \n",
    "    for i in range(matrix.shape[0]):\n",
    "\n",
    "        for j in range(i, matrix.shape[0]):\n",
    "            dist = np.sum((matrix[i] - matrix[j])**2)\n",
    "            total += dist\n",
    "    \n",
    "    return total/num\n",
    "\n",
    "def get_representatives(clusters, cluster_groups):\n",
    "    \n",
    "    representatives = []\n",
    "    \n",
    "    for i, cluster in enumerate(clusters):\n",
    "        \n",
    "        min_dist = np.inf\n",
    "        index = -1\n",
    "        for j, v in enumerate(cluster_groups[i]):\n",
    "            dist = np.sum((v-cluster)**2)\n",
    "            if dist < min_dist:\n",
    "                min_dist=dist\n",
    "                index = j\n",
    "        representatives.append(cluster_groups[i][index])\n",
    "    return representatives\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_clustering(vectors, num_clusters=10, iterations=10):\n",
    "    \n",
    "    #initialize clusters to random points\n",
    "    clusters = vectors[np.random.choice(vectors.shape[0], num_clusters, replace=False)]\n",
    "    new_clusters = clusters\n",
    "    for j in range(iterations):\n",
    "        clusters = new_clusters\n",
    "        # list of vectors belonging to each cluster\n",
    "        cluster_groups = [[] for cluster in clusters]\n",
    "        for i in range(vectors.shape[0]):\n",
    "            vector = vectors[i]\n",
    "\n",
    "            min_dist = np.inf\n",
    "            index = -1\n",
    "            for i,cluster in enumerate(clusters):\n",
    "                dist = np.sum( (vector-cluster)**2)\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist \n",
    "                    index = i\n",
    "            \n",
    "            # add to group of closest cluster\n",
    "            cluster_groups[index].append(vector)\n",
    "        print(\"Iteration:\", j, [len(c) for c in cluster_groups])\n",
    "        new_clusters = [mean_vector(group) for group in cluster_groups]\n",
    "        \n",
    "        diff = [new for i,new in enumerate(new_clusters) if not np.array_equal(new, clusters[i])]\n",
    "        if len(diff)==0:\n",
    "            break\n",
    "    return clusters, cluster_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fantastically slow implmentation, mainly due to\n",
    "# vectors not being hashable, and looking \n",
    "def dbscan_clustering(vectors, min_points=4, radius=0.5):\n",
    "    # normalize for densities\n",
    "    mean=np.mean(vectors)\n",
    "    std=np.std(vectors)\n",
    "    vectors = (vectors-mean)/std\n",
    "    \n",
    "    indices = set(range(vectors.shape[0]))\n",
    "\n",
    "    pq = PriorityQueue()\n",
    "    \n",
    "    # dict of index: [indices] representing cluster relations\n",
    "    \n",
    "    # O(n^2)\n",
    "    cluster_groups = {}\n",
    "    while not len(indices)==0:\n",
    "        for e in indices:\n",
    "            break\n",
    "        cluster_groups[e] = [e]\n",
    "        pq.put(e)\n",
    "        while not pq.empty():\n",
    "            index = pq.get()\n",
    "            indices.remove(index)\n",
    "            vector = vectors[index]\n",
    "            \n",
    "            near = []\n",
    "            for other in indices:\n",
    "                if other in cluster_groups[index]:\n",
    "                    continue\n",
    "                \n",
    "                other_vec = vectors[other]\n",
    "                dist = np.sum((vector - other_vec)**2)\n",
    "                if dist < radius ** 2:\n",
    "                    # mark visited\n",
    "                    cluster_groups[index].append(other)\n",
    "                    cluster_groups[other] = cluster_groups[index] # watch for pointing to the right place\n",
    "                    near.append(other)\n",
    "            if len(near) >= min_points - 1: #include the point that put it in the queue\n",
    "                for n in near:\n",
    "                    pq.put(n)\n",
    "    # O(n^2)       \n",
    "    groups = []\n",
    "    for index, cluster in cluster_groups.items():\n",
    "        same = [g for g in groups if set(g)==set(cluster)]\n",
    "        if len(same)==0:\n",
    "            groups.append(cluster)\n",
    "    return [ [vectors[i]*std+mean for i in g] for g in groups  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = reduce_embeddings(embed_text(\"bible.txt\"), components=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters, groups = kmeans_clustering(embeddings, num_clusters=20, iterations=40)\n",
    "reps = get_representatives(clusters, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(embeddings))\n",
    "groups = dbscan_clustering(embeddings,min_points=4, radius=15)\n",
    "print(mean_distances(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(groups)) # number of clusters from dbscan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
